---
title: Insert title here
key: 50e4433594a59ebf625955d98e82a7c3

---
## Voting

```yaml
type: "TitleSlide"
key: "d65f0a5f33"
```

`@lower_third`

name: RomÃ¡n de las Heras
title: Data Scientist, SAP / Agile Solutions


`@script`
In this second lesson, you are going to learn how to apply the first of the simple ensemble methods: voting.


---
## Ask the Audience

```yaml
type: "TwoColumns"
key: "078b076872"
```

`@part1`
![ask-the-audience.gif](http://assets.datacamp.com/production/repositories/3910/datasets/b22184509408340ea82c00b113c165a41581cf3f/72344_4.gif) {{1}}


`@part2`
**Wisdom of Crowds** {{2}}

- Collective intelligence.{{3}}
- Large group of individuals >= Single expert. {{4}}
- Less noise and bias. {{5}}
- Problem solving, decision making, innovation, and prediction. {{6}}


`@script`
Maybe you have seen contest shows, like "Who wants to be a Millionaire". When a contestant doesn't know the right answer or is not sure about it, there is a "life-line" option to poll the audience. Usually, the participant chooses the most voted answer, hoping it is the correct one.
In fact, according to stats from TV studios, the audience predicts the correct answer more than ninety percent of the time! How is that possible?
Well, this is thanks to the concept known as _Wisdom of Crowds_. It refers to the collective intelligence or opinion based on a group of individuals instead of a single expert. The idea is the aggregated opinion of the crowd can be as good as, and usually superior, to the answer of any individual, even that of an expert. This is a very useful technique commonly applied to problem solving, decision making, innovation, and prediction. The last one is of our interest.


---
## Majority Voting

```yaml
type: "TwoColumns"
key: "17d87ea273"
```

`@part1`
**Properties**

- Classification problems {{1}}
- Majority Voting: Mode {{2}}
- Odd number of classifiers (3+) {{3}}


`@part2`
**Wise Crowd Characteristics:** 

- Diverse: different algorithms or datasets {{4}}
- Independent and uncorrelated {{5}}
- Use individual knowledge {{6}}
- Aggregate individual predictions {{7}}


`@script`
Keep in mind that this is a technique which can only be applied to classification problems.
As the name implies, this technique combines the output of many classifiers by using a majority voting approach. In other words, the combined prediction will be the mode of the individual predictions.
In order to avoid the ambiguous case of having a multi-modal prediction, it is recommended to use an odd amount of classifiers. Therefore, we need at least three base classifiers, and when feasible, use five or more.
There are some characteristics you need in your "crowd" for voting ensemble to be effective. First off, it needs to be diverse: either because you are using different algorithms or different datasets.
Secondly, the predictions need to be independent and uncorrelated from the rest. Next, each model should be able to make its own prediction using only individual information. Finally, the ensemble model should aggregate individual predictions into a collective one.


---
## Voting Ensemble in Python from Scratch

```yaml
type: "TwoColumns"
key: "c9785f94b0"
```

`@part1`
**Build and Train the individual models** {{1}}
```
# Create the individual models
clf_knn = KNeighborsClassifier(5)
clf_dt = DecisionTreeClassifier()
clf_lr = LogisticRegression()

# Fit the models
clf_knn.fit(x_train, y_train)
clf_dt.fit(x_train, y_train)
clf_lr.fit(x_train, y_train)
``` {{2}}

```
# Output from stats.mode
ModeResult(mode=array([1]), 
           count=array([2]))
``` {{5}}


`@part2`
** Combine the predictions using Voting** {{3}}
```
# Make the invidual predictions
pred_knn = clf_knn.predict(x_test)
pred_dt = clf_dt.predict(x_test)
pred_lr = clf_lr.predict(x_test)

# Combine the predictions using voting
from scipy import stats

y_pred = []
for i in range(x_test.shape[0]):
    individual_preds = np.array([
       pred_knn[i], 
       pred_dt[i], 
       pred_lr[i]])
    combined_pred = stats.mode(
       individual_preds)[0][0]
    y_pred.insert(i, combined_pred)
``` {{4}}


`@script`
We can build a voting classifier from scratch in Python. First, we need to create our individual models. For example, here we are instantiating a 5-nearest neighbor classifier, a decision tree, and a logistic regression.
Then, we individually fit each of the models. In this case, we are using the same dataset.
Once having fitted the individual classifiers, we can combine their predictions using Voting. First, we make the individual predictions. And then, we combine those predictions by taking the mode, which can be calculated using the stats module from the scipy library. We gather the individual prediction into a numpy array, as the mode function expects this as a parameter. Notice how we use a double indexing for the mode, as this function returns an array of two arrays: the mode and the count, respectively.


---
## Voting Ensemble in Python using Scikit-learn

```yaml
type: "TwoColumns"
key: "4312ef5444"
```

`@part1`
**VotingClassifier** {{1}}

```
from sklearn.ensemble import 
     VotingClassifier
``` {{2}}

```
clf_voting = VotingClassifier(
    estimators=[
       ('label1', clf_1), 
       ('label2', clf_2),
       ...
       ('labelN', clf_N)])
``` {{3}}

- **Unfitted** classifiers {{4}}


`@part2`
```
# Create the individual models
clf_knn = KNeighborsClassifier(5)
clf_dt = DecisionTreeClassifier()
clf_lr = LogisticRegression()

# Create voting classifier
clf_voting = VotingClassifier(
    estimators=[
       ('knn', clf_knn), 
       ('dt', clf_dt), 
       ('lr', clf_lr)])

# Fit it to the training data
clf_voting.fit(x_train, y_train)

# Predict using the voting classifier
y_pred = clf_voting.predict(x_test)
``` {{5}}


`@script`
We could think of encapsulating this cumbersome task in a function, receiving a list of classifiers and returning a voting classifier. In fact, scikit-learn  already provides this functionality with the VotingClassifier class, which can be used by importing it from the ensemble scikit-learn module. The first parameter is a list (string, estimator) tuples. Those strings, work as labels or identifiers for each of the classifiers. These must be unfitted estimators, as the votingClassifier will take care of that.
Using the same three classifiers as before, we can create a VotingClassifier. This model can be fitted and make predictions like any other, using the fit and predict functions.


---
## Let's give it a try!

```yaml
type: "FinalSlide"
key: "11ac25944b"
```

`@script`
Let's give a try to what we have learned and build our first ensemble models using voting!

